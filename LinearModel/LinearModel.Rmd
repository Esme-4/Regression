---
title: "General Linear Model"
author: "F.A. Barrios<br><small>Instituto de Neurobiología, UNAM<br></small>"
date: "<small>`r Sys.Date()`</small>"
output:
  rmdformats::robobook:
    highlight: kate
  pdf_document: default
description: "to prepare Class2020 presentations"
---


```{r setup, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=FALSE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

```{r message = FALSE}
library(tidyverse)
library(here)
library(wesanderson)
library(rstatix)
library(HSAUR2)
library(multcomp)
library(emmeans)

```

# General Linear Model (GLM)

## Linear Regression

The term "regression" was introduced by Francis Galton (Darwin's nephew) during the XIX century to describe a biological phenomenon.  The heights of the descendants of tall ancestors have the tendency to "return", come back, to the normal average high in the population, known as the regression to the media. (Mr. Galton was an Eugenics supporter)

## Examples for "simple" linear regression

The general equation for the straight line is $y = mx + b_0$, this form is the "slope, intersection form". The slope is the rate of change the gives the change in $y$ for a unit change in $x$. Remember that the slope formula for two pair of points $(x_1, y_1)$ and $(x_2, y_2)$ is:
$$ m = \frac{(y_2 - y_1)}{(x_2 - x_1)}$$
## Example of linear regression  

**From the chap 9 Daniel** Després et al. point out that the topography of adipose tissue (AT) is associated with metabolic complications considered as risk factors for cardiovascular disease. It is important, they state, to measure the amount of intraabdominal AT as part of the evaluation of the cardiovascular disease risk of an individual. Computed tomography (CT), the only available technique that precisely and reliably measures the amount of deep abdominal AT, however, is costly and requires irradiation of the subject. In addition, the technique is not available to many physicians. Després et al. conducted a study to develop equations to predict the amount of deep abdominal AT from simple anthropometric measurements. Their subjects were men between the ages of 18 and 42 years who were free from metabolic disease that would require treatment. Among the measurements taken on each subject were deep abdominal AT obtained by CT and waist circumference as shown in EXA_C09_S03_01.csv A question of interest is how well one can predict and estimate deep abdominal AT from knowledge of the waist circumference. This question is typical of those that can be answered by means of regression analysis. Since deep abdominal AT is the variable about which we wish to make predictions and estimations, it is the dependent variable. The variable waist measurement, knowledge of which will be used to make the predictions and estimations, is the independent variable.

```{r}
# load the data file
Exa9.3 = read_csv("~/Dropbox/GitHub/Regression/DataSets/ch09_all/EXA_C09_S03_01.csv",
         show_col_types = FALSE)
         
names(Exa9.3)

plot(Y ~ X, data = Exa9.3, pch = 20)

# horizontal line and vertical line
Ybar=mean(Exa9.3$Y)
Xbar=mean(Exa9.3$X)

abline(h=Ybar, col = 2, lty = 2)
abline(v=Xbar, col = 2, lty = 2)

# simple linear model
Lin9.3 = lm(Y ~ X, data=Exa9.3)
summary(Lin9.3)

# linear model plot
abline(Lin9.3, col=2)
```
Response variable $Y$ is a random variable that is measured and has a distribution with expected value $E(Y|x)$ given a set of independent variables $x$.
$$Y_j (j=1, . . . , J)$$
for a set of $x_{jl}$ predictor variables (or independent variables) defined as vectors for each $j$
$$ x_{jl} (l=1, . . . , L)$$
with $L(L<J)$, a general linear model with an error function $\epsilon_j$ can be expressed:
$$Y_j = x_{j1}\beta_1 + x_{j2}\beta_2 + x_{j3}\beta_3 + . . . + x_{jL}\beta_L + \epsilon_j$$
with $\epsilon_j$ an independent variable identically distributed to the Normal with mean equal to zero.
$$\epsilon_j \approx N(0,\sigma^2)_{iid}$$

## Linear Regresion (Chap 4, Vittinghoff et all.)

Example of simple linear regression: exercise and glucose Glucose levels above 125 mg/dL are diagnostic of diabetes, while 100-125 mg/dL signal increased risk.
Data from HERS (public data) has baseline of glucose levels among 2,032 participants in a clinical trial of Hormone Therapy (HT). Women with diabetes are excluded, to study if the exercise might help prevent progression to diabetes.

```{r}
# ~/Dropbox/GitHub/Regression
hers <- read_csv("DataRegressBook/Chap3/hersdata.csv")

# hers data structure
hers_nodi <- filter(hers, diabetes == "no")
hers_nodi_Fit <- lm(glucose ~ exercise, data = hers_nodi)
# the linear model results can be printed using summary
summary(hers_nodi_Fit)
```

Simple linear regression model shows coefficient estimate ($\beta_1$) for exercise shows that average baseline glucose levels were about 1.7mg/dL lower among women who exercised at least three times a week than among women who exercised less.

## For a multiple linear model

There are models to regress several predictor variables to relate several random independent variables.

$$y_i = E[y_i|x_i] + \epsilon_i$$
$$Y = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \dots + \beta_p x_{p}$$
Multiple linear regression model coefficients, the betas, give the change in $E[Y|x]$ for an increase of one unit on the predictor $x_j$ , holding other factors in the model constant; each of the estimates is adjusted for the effects of all the other predictors. As in the simple linear model the intercept $\beta_0$ (beta zero) gives the value $E[Y|x]$ when all the predictors are equal to zero. Example of multiple linear model estimate is done with:  glucose ~ exercise + age + drinkany + BMI.

In general in R we can write:$Y = \beta_1 variable_1 + \beta_2 variable_2 + \beta_3 variable_3 + \beta_4 variable_4$ for a multiple linear model, in this case four regressors.

```{r}
hers_nodi_multFit <- lm(glucose ~ exercise + age + drinkany + BMI, data = hers_nodi)
# the linear model results can be printed using summary
summary(hers_nodi_multFit)
```

# The multiple regression model (4.2)

In the multiple regression model the expected value $E[y \mid x]$ (expected value of the response function y given the vector x) is
$$E[y \mid x] = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \cdots + \beta_p x_p$$

where $x$ represents the collection of $p$ predictors $x_1, x_2, \ldots x_p$ in the model, and the $\beta$ are the corresponding regression coefficients.

```{r}
# Chap 4 4.2 Multiple linear regresor model and to obtain the table 4.2 with multiple linear model

hers_nodi_Fit2 <- lm(glucose ~ exercise + age + drinkany + BMI, data = hers_nodi)
S(hers_nodi_Fit2)
confint(hers_nodi_Fit2)
#
```

in a multiple regression model that also includes —that is, adjusts for—age, alcohol use (drinkany), and BMI, average glucose is estimated to be only about 1 mg/dL lower among women who exercise, holding the other three factors constant. The multipredictor model also shows that average glucose levels are about 0.7 mg/dL higher among alcohol users than among nonusers. Average levels also increase by about 0.5 mg/dL per unit increase in BMI, and by 0.06 mg/dL for each additional year of age. Each of these associations is statistically significant after adjustment for the other predictors in the model.  

## Interpretation of Adjusted Regression Coefficients  

The coefficient $\beta_j, j=1, \ldots ,p$ gives the change in $E[y \mid x]$ for an increase of one unit in predictor $x_j$ , holding other factors in the model constant; each of the estimates is adjusted for the effects of all the other predictors. As in the simple linear model, the intercept $\beta_0$ gives the value of $E[y \mid x]$ when all the predictors are equal to zero.  

## Generalization of R-squared and r  

The coefficient of determination $R^2$ is the proportion of the total variability of the outcome that can be accounted for by the predictors. And the multiple correlatio coefficient $r = \sqrt{R^2}$  represents the correlation between the outcome $y$ and the fitted values $\hat{y}$.  

# Categorical Predictors  

Predictors in both simple and multiple predictor regression models can be binary, categorical, or discrete numeric, as well as continuous numeric.  

## Binary Predictors  

Binary predictors, a group with a characteristic and other group with out the characteristic, can be coded with a dummy variable, an indicator or dummy variable that can take value "1" for the group with the chracteristic and "0" for the group without the characteristic. With this coding, the regression coefficient corresponding to this variable has a straightforward interpretation as the increase or decrease in average outcome levels in the group with the characteristic, with respect to the reference group. With this coding for binary variables 1 = yes and 0 = no $\beta_0$ is the average of the baseline variable and $\beta_0 + \beta_1$ is related to the value for the "yes" condition ("yes" + average).  

## Multilevel Categorical Predictors (4.3)  

The 2,763 women in the HERS cohort also responded to a question about how physically active they considered themselves compared to other women their age. The five-level response variable "physact" ranged from “much less active” to “much more active,” and was coded in order from 1 to 5. This is an example of an ordinal variable. Multilevel categorical variables can also be nominal, in the sense that there is no intrinsic ordering in the categories. Examples include ethnicity, marital status, occupation, and geographic region. With nominal variables, it is even clearer that the numeric codes often used to represent the variable in the database cannot be treated like the values of a numeric variable.  
Categorical variables are easily accommodated in multipredictor linear and other regression models, using indicator or dummy variables. As with binary variables, where two categories are represented in the model by a single indicator variable, categorical variables with $K \leq 2$ levels are represented by $K - 1$ indicators, one for each of level of the variable except a baseline or reference level.  

```{r}
# Chap 4  4.3 Categorical predictors
# we are using the same file hers <- read_csv("DataRegressBook/Chap3/hersdata.csv")
# Multilevel categorical predictors using the linear model for women without diebetes
# IMPORTANT compare with table 4.4 Regression of physical activity on glucose

hers_nodi <- mutate(hers_nodi, physact = factor(physact, levels=c("much less active","somewhat less active","about as active","somewhat more active","much more active")))
levels(hers_nodi$physact)
ggplot(data = hers_nodi, mapping = aes(x = physact, y = glucose)) + geom_boxplot(na.rm = TRUE)
glucose_fit_act <- lm(glucose ~ physact, data = hers_nodi)
#
Anova(glucose_fit_act, type="II")
S(glucose_fit_act)
layout(matrix(1:4, nrow = 2))
plot(glucose_fit_act)

# To compute the estimates marginal means for specified factors or factor combinations in a linear model
glucose_emmeans <- emmeans(glucose_fit_act, "physact")
summary(glucose_emmeans)
#
```

the corresponding $\beta_i$ have a straightforward interpretation. For the moment, consider a simple regression model in which the five levels of "physact"
are the only predictors:
$$E[glucose \mid x] = \beta_0 + \beta_2 SomewhatLessActive + \beta_3 AboutAsActive + \beta_4 SomewhatMoreActive + \beta_5 MuchMoreActive$$


## Multiple linear model, with interactions

In general in R we can write the interaction term as the product of the regressors that we are studying the interaction:$variable_1:varible_2$ for a multiple linear model with two regressors and interaction the equation looks like:

$$Y=\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2$$

# Multiple Linear Model, HSAUR2 example

This are only additive linear terms to explain a random response variable $Y$ and the adjusted parameters are the $\beta_i$ of the independent variables or predictors. These variables are random too. They can be numbers and factors. Example of multiple linear regression using the clouds data clouds from HSAUR books.

```{r}
library(wordcloud)
data(clouds)
head(clouds)

# looking the data for rainfall
boxplot(rainfall ~ seeding, data = clouds)
# boxplot(rainfall ~ echomotion, data=clouds)
layout(matrix(1:2, ncol = 2))
boxplot(rainfall ~ seeding, data = clouds, ylab = "Rainfall", xlab = "Seeding")
boxplot(rainfall ~ echomotion, data = clouds, ylab = "Rainfall", xlab = "Echo Motion")
# 
layout(matrix(1:4, nrow = 2))
plot(rainfall ~ time, data = clouds)
plot(rainfall ~ cloudcover, data = clouds)
plot(rainfall ~ sne, data = clouds, xlab="S-Ne criterion")
plot(rainfall ~ prewetness, data = clouds)

#
clouds_formula <- rainfall ~ seeding + seeding:(sne+cloudcover+prewetness+echomotion) + time
Xstar <- model.matrix(clouds_formula, data = clouds)
attr(Xstar, "contrasts")
clouds_lm <- lm(clouds_formula, data = clouds)
summary(clouds_lm)

layout(matrix(1:1, nrow = 1))
# to list the betas* with the:
betaStar <- coef(clouds_lm)
betaStar
# to understand the relation of seeding and sne
psymb <- as.numeric(clouds$seeding)
plot(rainfall ~ sne, data = clouds, pch = psymb, xlab = "S-Ne criterion")
abline(lm(rainfall ~ sne, data = clouds, subset = seeding == "no"))
abline(lm(rainfall ~ sne, data = clouds, subset = seeding == "yes"), lty = 2)
legend("topright", legend = c("No seeding", "Seeding"), pch = 1:2, lty = 1:2, bty = "n")
#
# and the Covariant matrix Cov(beta*) with:
VbetaStar <- vcov(clouds_lm)
# Where the square roots of the diagonal elements are the standart errors 
sqrt(diag(VbetaStar))
clouds_resid <- residuals(clouds_lm)
clouds_fitted <- fitted(clouds_lm)
# residuals and the fitted values can be used to construct diagnostic plot
plot(clouds_fitted, clouds_resid, xlab = "Fitted values", ylab = "Residuals", type = "n", ylim = max(abs(clouds_resid)) * c(-1, 1))
abline(h = 0, lty = 2)
textplot(clouds_fitted, clouds_resid, words = rownames(clouds), new = FALSE)
qqnorm(clouds_resid, ylab = "Residuals")
qqline(clouds_resid)
```