---
title: "Clustering algorithms"
author: "Michael Jeziorski"
date: "5 May 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(factoextra)
library(car)
```

## Differences between PCA and clustering

Because both principal component analysis and clustering are directed at finding patterns in a dataset with the possible goal of providing a means to classify, they may appear identical, but have important differences.  The principles of tidy data, in which each column in a dataframe represents a variable and each row represents an observation, help illustrate the difference.  PCA is intended to reduce the *variables* to a small number that will still account for much of the variation in the data.  We can use the results to make inferences about how the data points are grouped, but those inferences are not a part of PCA itself.  

In contrast, clustering paradigms seek to group *observations* such that closely-related observations are within the same group while the separation between groups is large.  Both "closely-related" and "large separation" are subjective parameters, so the method used to cluster data contains certain a priori assumptions.  Furthermore, the number of subgroups ultimately generated can vary depending on how the analysis is applied.  

## Hierarchical clustering

Hierarchical clustering proceeds in a sequence of steps, leading to a tree-like network (dendrogram) in which all observations are classified.  This can be done in one of two ways:  agglomerative (bottom-up) clustering or divisive (top-down) clustering.  In agglomerative clustering, all observations are separate before the start, and groups of observations are then formed based on similarity.  Based on the measure of distance used to determine similarity or difference, the algorithm can then create second-order groups (groups of groups), then third-order groups, and so on until all observations fit into the dendrogram.  This approach allows viewing of clustering at different hierarchical levels.  Divisive clustering starts with all observations in a single group and then divides them into smaller and smaller groups.  

An example of a hierarchical cluster is species taxonomy, in which all species are part of a single tree of life, but the number of clusters depends on what level we examine: genus, order, class, phylum, etc.  

## Partitional clustering

k-means clustering, a form of partitional clustering, divides the observations into a predetermined number of clusters (k) such that the variation within each cluster is minimized and the variation between the clusters is maximized.  The ideal number of clusters to be used is not determined by any set rule, although certain guidelines can be used to optimize the number of clusters.  Unlike hierarchical clustering, partitional clustering uses a single step to create clusters that is based on the distances between observations.  The result resembles the output from PCA, but the algorithm is evaluating observations, not variables.  

k-means clustering uses an iterative process to assign k centroids to the observations, group the data so that each observation is assigned to the closest centroid, then optimize the locations of the centroids to minimize the within-cluster distances.  As a result, it can be sensitive to outlier data points.  It also tends to create clusters of comparable size.  Modified versions of k-means clustering exist that use different approaches to determining distance between observations.  For this exercise, however, we will examine how k-means clustering works and can be optimized.  

### An example of partitional clustering

As an example, we can use the `mtcars` dataset that is loaded in with base R.  The table contains technical data from 1974 on 32 different car models from around the world and is a small but useful dataset to illustrate principles of k-means clustering.  

```{r}
glimpse(mtcars)
```

As in PCA, we will scale the data using the `scale()` function in base R.  

```{r}
mtcars_s <- scale(mtcars)
summary(mtcars_s)
```

Once the data are scaled, we want to determine the distance between observations.  There are many ways to measure distance in a matrix.  For the purposes of this exercise, we will use the Euclidean method.  `get_dist()` is a function in the `factoextra` package.  

```{r}
res_dist <- get_dist(mtcars_s, method = "euclidean")
fviz_dist(res_dist, lab_size = 8)
```

The `eclust()` function, also in the `factoextra` package, incorporates ggplot2 for visualization.  It allows several different clustering paradigms; we will use k-means.  The `nstart` argument sets the number of configurations for the algorithm to begin its analysis.  

```{r message=FALSE}
res_kmeans <- eclust(mtcars_s, "kmeans", k.max = 15, nstart = 25)
```

The function returned 14 clusters.  Is this the ideal number?  Some subjective decisions will be made in the analysis.  To have an idea of how different numbers of clusters lead to different interpretations of the data, we can assign different values to k and see how the groupings change.  This can be accomplished using a `for` loop that applies values of k between 2 and 15.  

```{r}
for (i in 2:15) {eclust(mtcars_s, "kmeans", k = i, nstart = 25)}
```

The `fviz_gap_stat()` function can offer an idea of the optimal number of clusters to use.  A similar value is stored in the `nbclust` element of the object generated by `eclust()`.

```{r}
fviz_gap_stat(res_kmeans$gap_stat)
res_kmeans$nbclust
```

```{r}
fviz_silhouette(res_kmeans)
```

### An example of hierarchical clustering

```{r}
res_hc <- mtcars %>%
  scale() %>%                                              # Scale the data
  dist(method = "euclidean") %>%                           # Compute dissimilarity matrix
  hclust(method = "ward.D2")                               # Compute hierarchical clustering
fviz_dend(res_hc, k = 10,                                  # Use 10 groups
          cex = 0.5,                                       # label size
          color_labels_by_k = TRUE,                        # color labels by groups
          rect = TRUE                                      # Add rectangles around groups
          )
```

## `mtcars` analysis repeated using only columns `mpg:qsec`

```{r}
mtcars_s_lim <- mtcars %>% select(mpg:qsec) %>% scale()
res_dist_lim <- get_dist(mtcars_s_lim, method = "euclidean")
fviz_dist(res_dist_lim, lab_size = 8)
res_lim_kmeans <- eclust(mtcars_s_lim, "kmeans", k.max = 15, nstart = 25)
fviz_gap_stat(res_lim_kmeans$gap_stat)
```

```{r}
for (i in 2:15) {eclust(mtcars_s_lim, "kmeans", k = i, nstart = 25)}
```

## Work in progress below

### The tidymodels package

```{r, warning=FALSE, message=FALSE}
library(tidymodels)

set.seed(27)                    # ensure a reproducible example

centers <- tibble(
  cluster = factor(1:3), 
  num_points = c(100, 150, 50),  # number points in each cluster
  x1 = c(5, 0, -3),              # x1 coordinate of cluster center
  x2 = c(-1, 1, -2)              # x2 coordinate of cluster center
)

labelled_points <- 
  centers %>%
  mutate(
    x = map2(num_points, x1, rnorm),
    y = map2(num_points, x2, rnorm)
  ) %>% 
  select(-num_points, -x1, -x2) %>% 
  unnest(cols = c(x, y))

ggplot(labelled_points, aes(x, y, color = cluster)) +
  geom_point(alpha = 0.3)
```




```{r}
points <- 
  labelled_points %>% 
  select(-cluster)

kclust <- kmeans(points, centers = 3)
kclust
summary(kclust)
```

